{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Evaluation\n",
    "\n",
    "In this assignment you will train several models and evaluate how effectively they predict instances of fraud using data based on [this dataset from Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud).\n",
    " \n",
    "Each row in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` through `V28` as well as `Amount` which is the amount of the transaction. \n",
    " \n",
    "The target is stored in the `class` column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Import the data from `fraud_data.csv`. What percentage of the observations in the dataset are instances of fraud?\n",
    "\n",
    "*This function should return a float between 0 and 1.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    fraud = pd.read_csv('fraud_data.csv')\n",
    "    \n",
    "    tot = len(fraud)\n",
    "    pos = len(fraud[fraud['Class'] == 1])\n",
    "\n",
    "    print(tot,pos)\n",
    "    \n",
    "    return pos/tot # Return your answer\n",
    "#answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
    "\n",
    "*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "    y_dummy = dummy_majority.predict(X_test)\n",
    "    \n",
    "    return (accuracy_score(y_test, y_dummy),recall_score(y_test, y_dummy)) # Return your answer\n",
    "#answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?\n",
    "\n",
    "*This function should a return a tuple with three floats, i.e. `(accuracy score, recall score, precision score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99078171091445433, 0.375, 1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    svm = SVC(kernel='rbf').fit(X_train, y_train)\n",
    "    \n",
    "    y_svm = svm.predict(X_test)\n",
    "    \n",
    "    return (\n",
    "        accuracy_score(y_test,y_svm),\n",
    "        recall_score(y_test,y_svm),\n",
    "        precision_score(y_test,y_svm)\n",
    "        ) # Return your answer\n",
    "#answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Using the SVC classifier with parameters `{'C': 1e9, 'gamma': 1e-07}`, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.\n",
    "\n",
    "*This function should return a confusion matrix, a 2x2 numpy array with 4 integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5320,   24],\n",
       "       [  14,   66]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    svm = SVC(kernel='rbf',C=1e9, gamma=1e-07).fit(X_train, y_train)\n",
    "\n",
    "    y_scores_svm = svm.decision_function(X_test)\n",
    "    \n",
    "    thres = lambda x: 0 if x < -220 else 1\n",
    "    vthres = np.vectorize(thres)\n",
    "    y_probs = vthres(y_scores_svm)\n",
    "    #print(probs)\n",
    "    #y_svm = svm.predict(X_test)\n",
    "    \n",
    "    confusion = confusion_matrix(y_test, y_probs)\n",
    "    \n",
    "    return confusion #confusion# Return your answer\n",
    "#answer_four() # need to figure out the decision function part, rewatch lecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Train a logisitic regression classifier with default parameters using X_train and y_train.\n",
    "\n",
    "For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).\n",
    "\n",
    "Looking at the precision recall curve, what is the recall when the precision is `0.75`?\n",
    "\n",
    "Looking at the roc curve, what is the true positive rate when the false positive rate is `0.16`?\n",
    "\n",
    "*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    \n",
    "    lr = LogisticRegression().fit(X_train, y_train)\n",
    "    lr_predicted = lr.predict(X_test)\n",
    "    \n",
    "    y_scores_lr = lr.decision_function(X_test)\n",
    "    \n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "    #compare=pd.DataFrame({'precision':precision, 'recall':recall}) #, 'thresholds':thresholds})\n",
    "    #print(compare[(compare['precision'] > 0.74) & (compare['precision'] < 0.76)])\n",
    "    \n",
    "    #plt.figure(figsize=(15,15))\n",
    "    #plt.xlim([0.0, 1.01])\n",
    "    #plt.ylim([0.0, 1.01])\n",
    "    #plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "    #plt.xlabel('Precision', fontsize=16)\n",
    "    #plt.ylabel('Recall', fontsize=16)\n",
    "    #plt.axes().set_aspect('equal')\n",
    "    #plt.xticks(np.arange(0, 1, 0.05))\n",
    "    #plt.yticks(np.arange(0, 1, 0.05))\n",
    "    #plt.show()\n",
    "    \n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_scores_lr)\n",
    "    roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "    \n",
    "    #compare2=pd.DataFrame({'fpr_lr':fpr_lr, 'tpr_lr':tpr_lr})\n",
    "    #print(compare2[(compare2['fpr_lr'] > 0.15) & (compare2['fpr_lr'] < 0.21)])\n",
    "    \n",
    "    #plt.figure(figsize=(15,15))\n",
    "    #plt.xlim([-0.01, 1.00])\n",
    "    #plt.ylim([-0.01, 1.01])\n",
    "    #plt.plot(fpr_lr, tpr_lr, lw=3)\n",
    "    #plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "    #plt.axes().set_aspect('equal')\n",
    "    #plt.xticks(np.arange(0, 1, 0.05))\n",
    "    #plt.yticks(np.arange(0, 1, 0.05))\n",
    "    #plt.show()\n",
    "\n",
    "    return (0.825,0.950) # Return your answer\n",
    "#answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.01, 0.1, 1, 10, 100]`\n",
    "\n",
    "From `.cv_results_`, create an array of the mean test scores of each parameter combination. i.e.\n",
    "\n",
    "|      \t| `l1` \t| `l2` \t|\n",
    "|:----:\t|----\t|----\t|\n",
    "| **`0.01`** \t|    ?\t|   ? \t|\n",
    "| **`0.1`**  \t|    ?\t|   ? \t|\n",
    "| **`1`**    \t|    ?\t|   ? \t|\n",
    "| **`10`**   \t|    ?\t|   ? \t|\n",
    "| **`100`**   \t|    ?\t|   ? \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "*This function should return a 5 by 2 numpy array with 10 floats.* \n",
    "\n",
    "*Note: do not return a DataFrame, just the values denoted by '?' above in a numpy array.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': ({'C': 0.01, 'penalty': 'l1'}, {'C': 0.01, 'penalty': 'l2'}, {'C': 0.1, 'penalty': 'l1'}, {'C': 0.1, 'penalty': 'l2'}, {'C': 1, 'penalty': 'l1'}, {'C': 1, 'penalty': 'l2'}, {'C': 10, 'penalty': 'l1'}, {'C': 10, 'penalty': 'l2'}, {'C': 100, 'penalty': 'l1'}, {'C': 100, 'penalty': 'l2'}), 'split2_train_score': array([ 0.67934783,  0.7826087 ,  0.82065217,  0.82608696,  0.82608696,\n",
      "        0.82608696,  0.83695652,  0.83695652,  0.83695652,  0.83695652]), 'mean_score_time': array([ 0.01154208,  0.00842683,  0.02152673,  0.01573261,  0.01726238,\n",
      "        0.02810113,  0.00899291,  0.00619888,  0.00980457,  0.02846138]), 'mean_fit_time': array([ 0.13630144,  0.27647201,  0.19094539,  0.38142268,  0.30498497,\n",
      "        0.50771483,  0.50320443,  0.43437942,  0.93603619,  0.45147189]), 'std_fit_time': array([ 0.02339944,  0.02623218,  0.02354314,  0.01185907,  0.03304191,\n",
      "        0.05573472,  0.14386282,  0.07095647,  0.66949859,  0.03517778]), 'split1_train_score': array([ 0.67391304,  0.76630435,  0.79891304,  0.80434783,  0.80978261,\n",
      "        0.80978261,  0.81521739,  0.81521739,  0.81521739,  0.81521739]), 'mean_test_score': array([ 0.66666667,  0.76086957,  0.80072464,  0.80434783,  0.8115942 ,\n",
      "        0.8115942 ,  0.80797101,  0.8115942 ,  0.80797101,  0.80797101]), 'split1_test_score': array([ 0.69565217,  0.75      ,  0.83695652,  0.83695652,  0.83695652,\n",
      "        0.83695652,  0.83695652,  0.83695652,  0.83695652,  0.83695652]), 'split0_test_score': array([ 0.70652174,  0.80434783,  0.80434783,  0.82608696,  0.82608696,\n",
      "        0.82608696,  0.82608696,  0.83695652,  0.82608696,  0.82608696]), 'split2_test_score': array([ 0.59782609,  0.72826087,  0.76086957,  0.75      ,  0.77173913,\n",
      "        0.77173913,  0.76086957,  0.76086957,  0.76086957,  0.76086957]), 'param_penalty': masked_array(data = ['l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2' 'l1' 'l2'],\n",
      "             mask = [False False False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'param_C': masked_array(data = [0.01 0.01 0.1 0.1 1 1 10 10 100 100],\n",
      "             mask = [False False False False False False False False False False],\n",
      "       fill_value = ?)\n",
      ", 'mean_train_score': array([ 0.68115942,  0.77355072,  0.80615942,  0.8134058 ,  0.81884058,\n",
      "        0.81702899,  0.82427536,  0.82427536,  0.82427536,  0.82427536]), 'std_train_score': array([ 0.00677836,  0.00677836,  0.01024792,  0.00923735,  0.00677836,\n",
      "        0.00677836,  0.00923735,  0.00923735,  0.00923735,  0.00923735]), 'std_test_score': array([ 0.04887948,  0.03199913,  0.03116785,  0.03868507,  0.02852901,\n",
      "        0.02852901,  0.03360007,  0.03586774,  0.03360007,  0.03360007]), 'split0_train_score': array([ 0.69021739,  0.77173913,  0.79891304,  0.80978261,  0.82065217,\n",
      "        0.81521739,  0.82065217,  0.82065217,  0.82065217,  0.82065217]), 'rank_test_score': array([10,  9,  8,  7,  2,  2,  4,  1,  4,  4], dtype=int32), 'std_score_time': array([ 0.00128156,  0.00404836,  0.00994773,  0.00669381,  0.01008033,\n",
      "        0.02154344,  0.00500559,  0.00017938,  0.00349066,  0.0168462 ])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.66666667,  0.76086957],\n",
       "       [ 0.80072464,  0.80434783],\n",
       "       [ 0.8115942 ,  0.8115942 ],\n",
       "       [ 0.80797101,  0.8115942 ],\n",
       "       [ 0.80797101,  0.80797101]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    grid_values = {'penalty': ['l1', 'l2'],'C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "    grid_clf_rec = GridSearchCV(clf, param_grid = grid_values, scoring = 'recall')\n",
    "    grid_clf_rec.fit(X_train, y_train)\n",
    "    \n",
    "    #print(grid_clf_rec.cv_results_)\n",
    "    res = grid_clf_rec.cv_results_['mean_test_score']\n",
    "    \n",
    "    return res.reshape(5,2) # Return your answer\n",
    "#answer_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the following function to help visualize results from the grid search\n",
    "def GridSearch_Heatmap(scores):\n",
    "    %matplotlib notebook\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    sns.heatmap(scores.reshape(5,2), xticklabels=['l1','l2'], yticklabels=[0.01, 0.1, 1, 10, 100])\n",
    "    plt.yticks(rotation=0);\n",
    "\n",
    "#GridSearch_Heatmap(answer_six())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "5yX9Z",
   "launcher_item_id": "eqnV3",
   "part_id": "Msnj0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
